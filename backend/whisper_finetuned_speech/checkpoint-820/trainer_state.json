{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 41.0,
  "eval_steps": 500,
  "global_step": 820,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.25316455696202533,
      "grad_norm": 19.731046676635742,
      "learning_rate": 9.960000000000001e-05,
      "loss": 6.1131,
      "step": 5
    },
    {
      "epoch": 0.5063291139240507,
      "grad_norm": 9.917762756347656,
      "learning_rate": 9.910000000000001e-05,
      "loss": 5.4334,
      "step": 10
    },
    {
      "epoch": 0.759493670886076,
      "grad_norm": 6.141446590423584,
      "learning_rate": 9.86e-05,
      "loss": 5.2574,
      "step": 15
    },
    {
      "epoch": 1.0,
      "grad_norm": 8.291156768798828,
      "learning_rate": 9.81e-05,
      "loss": 4.5172,
      "step": 20
    },
    {
      "epoch": 1.2531645569620253,
      "grad_norm": 6.600955963134766,
      "learning_rate": 9.76e-05,
      "loss": 4.1484,
      "step": 25
    },
    {
      "epoch": 1.5063291139240507,
      "grad_norm": 6.438531398773193,
      "learning_rate": 9.71e-05,
      "loss": 3.8775,
      "step": 30
    },
    {
      "epoch": 1.759493670886076,
      "grad_norm": 7.008595943450928,
      "learning_rate": 9.66e-05,
      "loss": 3.6751,
      "step": 35
    },
    {
      "epoch": 2.0,
      "grad_norm": 10.986820220947266,
      "learning_rate": 9.61e-05,
      "loss": 3.5045,
      "step": 40
    },
    {
      "epoch": 2.2531645569620253,
      "grad_norm": 7.412761688232422,
      "learning_rate": 9.56e-05,
      "loss": 3.1169,
      "step": 45
    },
    {
      "epoch": 2.5063291139240507,
      "grad_norm": 6.435118675231934,
      "learning_rate": 9.51e-05,
      "loss": 2.523,
      "step": 50
    },
    {
      "epoch": 2.759493670886076,
      "grad_norm": 6.349609851837158,
      "learning_rate": 9.46e-05,
      "loss": 2.5105,
      "step": 55
    },
    {
      "epoch": 3.0,
      "grad_norm": 7.562082290649414,
      "learning_rate": 9.41e-05,
      "loss": 2.3013,
      "step": 60
    },
    {
      "epoch": 3.2531645569620253,
      "grad_norm": 5.024296283721924,
      "learning_rate": 9.360000000000001e-05,
      "loss": 2.2049,
      "step": 65
    },
    {
      "epoch": 3.5063291139240507,
      "grad_norm": 6.271437644958496,
      "learning_rate": 9.310000000000001e-05,
      "loss": 1.9153,
      "step": 70
    },
    {
      "epoch": 3.759493670886076,
      "grad_norm": 5.424072265625,
      "learning_rate": 9.260000000000001e-05,
      "loss": 1.7419,
      "step": 75
    },
    {
      "epoch": 4.0,
      "grad_norm": 8.219731330871582,
      "learning_rate": 9.21e-05,
      "loss": 1.8245,
      "step": 80
    },
    {
      "epoch": 4.253164556962025,
      "grad_norm": 5.420848369598389,
      "learning_rate": 9.16e-05,
      "loss": 1.7386,
      "step": 85
    },
    {
      "epoch": 4.506329113924051,
      "grad_norm": 6.561211585998535,
      "learning_rate": 9.11e-05,
      "loss": 1.6915,
      "step": 90
    },
    {
      "epoch": 4.759493670886076,
      "grad_norm": 5.888486862182617,
      "learning_rate": 9.06e-05,
      "loss": 1.3288,
      "step": 95
    },
    {
      "epoch": 5.0,
      "grad_norm": 5.378714084625244,
      "learning_rate": 9.010000000000001e-05,
      "loss": 1.3568,
      "step": 100
    },
    {
      "epoch": 5.253164556962025,
      "grad_norm": 3.669459819793701,
      "learning_rate": 8.960000000000001e-05,
      "loss": 1.3827,
      "step": 105
    },
    {
      "epoch": 5.506329113924051,
      "grad_norm": 5.72630500793457,
      "learning_rate": 8.910000000000001e-05,
      "loss": 1.3347,
      "step": 110
    },
    {
      "epoch": 5.759493670886076,
      "grad_norm": 4.334919452667236,
      "learning_rate": 8.86e-05,
      "loss": 1.291,
      "step": 115
    },
    {
      "epoch": 6.0,
      "grad_norm": 5.555732250213623,
      "learning_rate": 8.81e-05,
      "loss": 1.2163,
      "step": 120
    },
    {
      "epoch": 6.253164556962025,
      "grad_norm": 4.783658981323242,
      "learning_rate": 8.76e-05,
      "loss": 0.9949,
      "step": 125
    },
    {
      "epoch": 6.506329113924051,
      "grad_norm": 3.9472296237945557,
      "learning_rate": 8.71e-05,
      "loss": 0.9989,
      "step": 130
    },
    {
      "epoch": 6.759493670886076,
      "grad_norm": 6.2748613357543945,
      "learning_rate": 8.66e-05,
      "loss": 1.1515,
      "step": 135
    },
    {
      "epoch": 7.0,
      "grad_norm": 9.904277801513672,
      "learning_rate": 8.61e-05,
      "loss": 1.1233,
      "step": 140
    },
    {
      "epoch": 7.253164556962025,
      "grad_norm": 5.162744998931885,
      "learning_rate": 8.560000000000001e-05,
      "loss": 0.7695,
      "step": 145
    },
    {
      "epoch": 7.506329113924051,
      "grad_norm": 4.926236152648926,
      "learning_rate": 8.510000000000001e-05,
      "loss": 0.9844,
      "step": 150
    },
    {
      "epoch": 7.759493670886076,
      "grad_norm": 4.521048069000244,
      "learning_rate": 8.46e-05,
      "loss": 0.8624,
      "step": 155
    },
    {
      "epoch": 8.0,
      "grad_norm": 5.950946807861328,
      "learning_rate": 8.41e-05,
      "loss": 0.9666,
      "step": 160
    },
    {
      "epoch": 8.253164556962025,
      "grad_norm": 3.619189500808716,
      "learning_rate": 8.36e-05,
      "loss": 0.9169,
      "step": 165
    },
    {
      "epoch": 8.50632911392405,
      "grad_norm": 3.9922609329223633,
      "learning_rate": 8.31e-05,
      "loss": 0.7444,
      "step": 170
    },
    {
      "epoch": 8.759493670886076,
      "grad_norm": 3.806288242340088,
      "learning_rate": 8.26e-05,
      "loss": 0.7683,
      "step": 175
    },
    {
      "epoch": 9.0,
      "grad_norm": 4.615965843200684,
      "learning_rate": 8.21e-05,
      "loss": 0.855,
      "step": 180
    },
    {
      "epoch": 9.253164556962025,
      "grad_norm": 4.403070449829102,
      "learning_rate": 8.16e-05,
      "loss": 0.7444,
      "step": 185
    },
    {
      "epoch": 9.50632911392405,
      "grad_norm": 4.264815330505371,
      "learning_rate": 8.11e-05,
      "loss": 0.7635,
      "step": 190
    },
    {
      "epoch": 9.759493670886076,
      "grad_norm": 4.228424549102783,
      "learning_rate": 8.060000000000001e-05,
      "loss": 0.7226,
      "step": 195
    },
    {
      "epoch": 10.0,
      "grad_norm": 4.7410888671875,
      "learning_rate": 8.010000000000001e-05,
      "loss": 0.8572,
      "step": 200
    },
    {
      "epoch": 10.253164556962025,
      "grad_norm": 3.987682580947876,
      "learning_rate": 7.960000000000001e-05,
      "loss": 0.6141,
      "step": 205
    },
    {
      "epoch": 10.50632911392405,
      "grad_norm": 4.9741902351379395,
      "learning_rate": 7.910000000000001e-05,
      "loss": 0.6657,
      "step": 210
    },
    {
      "epoch": 10.759493670886076,
      "grad_norm": 3.880720615386963,
      "learning_rate": 7.860000000000001e-05,
      "loss": 0.716,
      "step": 215
    },
    {
      "epoch": 11.0,
      "grad_norm": 5.039191722869873,
      "learning_rate": 7.81e-05,
      "loss": 0.9415,
      "step": 220
    },
    {
      "epoch": 11.253164556962025,
      "grad_norm": 3.79884934425354,
      "learning_rate": 7.76e-05,
      "loss": 0.6818,
      "step": 225
    },
    {
      "epoch": 11.50632911392405,
      "grad_norm": 3.893779754638672,
      "learning_rate": 7.71e-05,
      "loss": 0.6508,
      "step": 230
    },
    {
      "epoch": 11.759493670886076,
      "grad_norm": 3.9087119102478027,
      "learning_rate": 7.66e-05,
      "loss": 0.7669,
      "step": 235
    },
    {
      "epoch": 12.0,
      "grad_norm": 5.033111572265625,
      "learning_rate": 7.61e-05,
      "loss": 0.6799,
      "step": 240
    },
    {
      "epoch": 12.253164556962025,
      "grad_norm": 4.1193318367004395,
      "learning_rate": 7.560000000000001e-05,
      "loss": 0.7777,
      "step": 245
    },
    {
      "epoch": 12.50632911392405,
      "grad_norm": 4.8066182136535645,
      "learning_rate": 7.510000000000001e-05,
      "loss": 0.6585,
      "step": 250
    },
    {
      "epoch": 12.759493670886076,
      "grad_norm": 3.9273762702941895,
      "learning_rate": 7.46e-05,
      "loss": 0.7028,
      "step": 255
    },
    {
      "epoch": 13.0,
      "grad_norm": 5.171333312988281,
      "learning_rate": 7.41e-05,
      "loss": 0.5188,
      "step": 260
    },
    {
      "epoch": 13.253164556962025,
      "grad_norm": 3.5931506156921387,
      "learning_rate": 7.36e-05,
      "loss": 0.7519,
      "step": 265
    },
    {
      "epoch": 13.50632911392405,
      "grad_norm": 3.880220890045166,
      "learning_rate": 7.31e-05,
      "loss": 0.5614,
      "step": 270
    },
    {
      "epoch": 13.759493670886076,
      "grad_norm": 9.333404541015625,
      "learning_rate": 7.26e-05,
      "loss": 0.6596,
      "step": 275
    },
    {
      "epoch": 14.0,
      "grad_norm": 4.482582092285156,
      "learning_rate": 7.21e-05,
      "loss": 0.5898,
      "step": 280
    },
    {
      "epoch": 14.253164556962025,
      "grad_norm": 4.006572723388672,
      "learning_rate": 7.16e-05,
      "loss": 0.6145,
      "step": 285
    },
    {
      "epoch": 14.50632911392405,
      "grad_norm": 4.768771171569824,
      "learning_rate": 7.11e-05,
      "loss": 0.5677,
      "step": 290
    },
    {
      "epoch": 14.759493670886076,
      "grad_norm": 4.9097819328308105,
      "learning_rate": 7.06e-05,
      "loss": 0.6334,
      "step": 295
    },
    {
      "epoch": 15.0,
      "grad_norm": 4.873103618621826,
      "learning_rate": 7.01e-05,
      "loss": 0.6508,
      "step": 300
    },
    {
      "epoch": 15.253164556962025,
      "grad_norm": 2.817187547683716,
      "learning_rate": 6.96e-05,
      "loss": 0.5345,
      "step": 305
    },
    {
      "epoch": 15.50632911392405,
      "grad_norm": 3.4253485202789307,
      "learning_rate": 6.91e-05,
      "loss": 0.769,
      "step": 310
    },
    {
      "epoch": 15.759493670886076,
      "grad_norm": 4.7428765296936035,
      "learning_rate": 6.860000000000001e-05,
      "loss": 0.4563,
      "step": 315
    },
    {
      "epoch": 16.0,
      "grad_norm": 6.161206245422363,
      "learning_rate": 6.81e-05,
      "loss": 0.616,
      "step": 320
    },
    {
      "epoch": 16.253164556962027,
      "grad_norm": 5.163196563720703,
      "learning_rate": 6.76e-05,
      "loss": 0.5866,
      "step": 325
    },
    {
      "epoch": 16.50632911392405,
      "grad_norm": 5.394714832305908,
      "learning_rate": 6.71e-05,
      "loss": 0.5692,
      "step": 330
    },
    {
      "epoch": 16.759493670886076,
      "grad_norm": 3.8219707012176514,
      "learning_rate": 6.66e-05,
      "loss": 0.5382,
      "step": 335
    },
    {
      "epoch": 17.0,
      "grad_norm": 4.495510578155518,
      "learning_rate": 6.610000000000001e-05,
      "loss": 0.5918,
      "step": 340
    },
    {
      "epoch": 17.253164556962027,
      "grad_norm": 3.9266839027404785,
      "learning_rate": 6.560000000000001e-05,
      "loss": 0.5335,
      "step": 345
    },
    {
      "epoch": 17.50632911392405,
      "grad_norm": 3.879481315612793,
      "learning_rate": 6.510000000000001e-05,
      "loss": 0.6093,
      "step": 350
    },
    {
      "epoch": 17.759493670886076,
      "grad_norm": 4.2228684425354,
      "learning_rate": 6.460000000000001e-05,
      "loss": 0.5076,
      "step": 355
    },
    {
      "epoch": 18.0,
      "grad_norm": 3.781595468521118,
      "learning_rate": 6.41e-05,
      "loss": 0.5496,
      "step": 360
    },
    {
      "epoch": 18.253164556962027,
      "grad_norm": 2.8097193241119385,
      "learning_rate": 6.36e-05,
      "loss": 0.4359,
      "step": 365
    },
    {
      "epoch": 18.50632911392405,
      "grad_norm": 2.181913375854492,
      "learning_rate": 6.31e-05,
      "loss": 0.3846,
      "step": 370
    },
    {
      "epoch": 18.759493670886076,
      "grad_norm": 5.420681476593018,
      "learning_rate": 6.26e-05,
      "loss": 0.8,
      "step": 375
    },
    {
      "epoch": 19.0,
      "grad_norm": 3.19446063041687,
      "learning_rate": 6.21e-05,
      "loss": 0.5039,
      "step": 380
    },
    {
      "epoch": 19.253164556962027,
      "grad_norm": 3.838487386703491,
      "learning_rate": 6.16e-05,
      "loss": 0.5951,
      "step": 385
    },
    {
      "epoch": 19.50632911392405,
      "grad_norm": 4.479564666748047,
      "learning_rate": 6.110000000000001e-05,
      "loss": 0.3834,
      "step": 390
    },
    {
      "epoch": 19.759493670886076,
      "grad_norm": 4.188119888305664,
      "learning_rate": 6.06e-05,
      "loss": 0.5256,
      "step": 395
    },
    {
      "epoch": 20.0,
      "grad_norm": 3.7134530544281006,
      "learning_rate": 6.0100000000000004e-05,
      "loss": 0.5719,
      "step": 400
    },
    {
      "epoch": 20.253164556962027,
      "grad_norm": 4.438872814178467,
      "learning_rate": 5.96e-05,
      "loss": 0.595,
      "step": 405
    },
    {
      "epoch": 20.50632911392405,
      "grad_norm": 4.223982810974121,
      "learning_rate": 5.91e-05,
      "loss": 0.5431,
      "step": 410
    },
    {
      "epoch": 20.759493670886076,
      "grad_norm": 3.619319200515747,
      "learning_rate": 5.86e-05,
      "loss": 0.4012,
      "step": 415
    },
    {
      "epoch": 21.0,
      "grad_norm": 5.0347795486450195,
      "learning_rate": 5.8099999999999996e-05,
      "loss": 0.4739,
      "step": 420
    },
    {
      "epoch": 21.253164556962027,
      "grad_norm": 4.086266040802002,
      "learning_rate": 5.76e-05,
      "loss": 0.466,
      "step": 425
    },
    {
      "epoch": 21.50632911392405,
      "grad_norm": 4.7071533203125,
      "learning_rate": 5.71e-05,
      "loss": 0.5208,
      "step": 430
    },
    {
      "epoch": 21.759493670886076,
      "grad_norm": 2.2246742248535156,
      "learning_rate": 5.66e-05,
      "loss": 0.4172,
      "step": 435
    },
    {
      "epoch": 22.0,
      "grad_norm": 3.021975517272949,
      "learning_rate": 5.610000000000001e-05,
      "loss": 0.5556,
      "step": 440
    },
    {
      "epoch": 22.253164556962027,
      "grad_norm": 3.338322401046753,
      "learning_rate": 5.560000000000001e-05,
      "loss": 0.383,
      "step": 445
    },
    {
      "epoch": 22.50632911392405,
      "grad_norm": 5.484958648681641,
      "learning_rate": 5.5100000000000004e-05,
      "loss": 0.4404,
      "step": 450
    },
    {
      "epoch": 22.759493670886076,
      "grad_norm": 3.4654996395111084,
      "learning_rate": 5.4600000000000006e-05,
      "loss": 0.5314,
      "step": 455
    },
    {
      "epoch": 23.0,
      "grad_norm": 6.263333320617676,
      "learning_rate": 5.410000000000001e-05,
      "loss": 0.583,
      "step": 460
    },
    {
      "epoch": 23.253164556962027,
      "grad_norm": 3.7556071281433105,
      "learning_rate": 5.360000000000001e-05,
      "loss": 0.5353,
      "step": 465
    },
    {
      "epoch": 23.50632911392405,
      "grad_norm": 5.636689186096191,
      "learning_rate": 5.31e-05,
      "loss": 0.5087,
      "step": 470
    },
    {
      "epoch": 23.759493670886076,
      "grad_norm": 4.9764533042907715,
      "learning_rate": 5.2600000000000005e-05,
      "loss": 0.511,
      "step": 475
    },
    {
      "epoch": 24.0,
      "grad_norm": 3.1000936031341553,
      "learning_rate": 5.2100000000000006e-05,
      "loss": 0.3203,
      "step": 480
    },
    {
      "epoch": 24.253164556962027,
      "grad_norm": 4.975384712219238,
      "learning_rate": 5.16e-05,
      "loss": 0.3753,
      "step": 485
    },
    {
      "epoch": 24.50632911392405,
      "grad_norm": 3.3910858631134033,
      "learning_rate": 5.11e-05,
      "loss": 0.4391,
      "step": 490
    },
    {
      "epoch": 24.759493670886076,
      "grad_norm": 3.946648120880127,
      "learning_rate": 5.0600000000000003e-05,
      "loss": 0.5504,
      "step": 495
    },
    {
      "epoch": 25.0,
      "grad_norm": 5.564515113830566,
      "learning_rate": 5.0100000000000005e-05,
      "loss": 0.4802,
      "step": 500
    },
    {
      "epoch": 25.253164556962027,
      "grad_norm": 2.202059030532837,
      "learning_rate": 4.96e-05,
      "loss": 0.2613,
      "step": 505
    },
    {
      "epoch": 25.50632911392405,
      "grad_norm": 4.453385353088379,
      "learning_rate": 4.91e-05,
      "loss": 0.4992,
      "step": 510
    },
    {
      "epoch": 25.759493670886076,
      "grad_norm": 4.044295310974121,
      "learning_rate": 4.86e-05,
      "loss": 0.6249,
      "step": 515
    },
    {
      "epoch": 26.0,
      "grad_norm": 5.920329570770264,
      "learning_rate": 4.8100000000000004e-05,
      "loss": 0.3996,
      "step": 520
    },
    {
      "epoch": 26.253164556962027,
      "grad_norm": 4.710367202758789,
      "learning_rate": 4.76e-05,
      "loss": 0.4486,
      "step": 525
    },
    {
      "epoch": 26.50632911392405,
      "grad_norm": 3.373429775238037,
      "learning_rate": 4.71e-05,
      "loss": 0.36,
      "step": 530
    },
    {
      "epoch": 26.759493670886076,
      "grad_norm": 4.80109167098999,
      "learning_rate": 4.660000000000001e-05,
      "loss": 0.5217,
      "step": 535
    },
    {
      "epoch": 27.0,
      "grad_norm": 4.610437870025635,
      "learning_rate": 4.61e-05,
      "loss": 0.4185,
      "step": 540
    },
    {
      "epoch": 27.253164556962027,
      "grad_norm": 3.766343593597412,
      "learning_rate": 4.5600000000000004e-05,
      "loss": 0.418,
      "step": 545
    },
    {
      "epoch": 27.50632911392405,
      "grad_norm": 3.3093619346618652,
      "learning_rate": 4.5100000000000005e-05,
      "loss": 0.3221,
      "step": 550
    },
    {
      "epoch": 27.759493670886076,
      "grad_norm": 3.6464855670928955,
      "learning_rate": 4.46e-05,
      "loss": 0.4497,
      "step": 555
    },
    {
      "epoch": 28.0,
      "grad_norm": 4.520599842071533,
      "learning_rate": 4.41e-05,
      "loss": 0.5026,
      "step": 560
    },
    {
      "epoch": 28.253164556962027,
      "grad_norm": 4.385642051696777,
      "learning_rate": 4.36e-05,
      "loss": 0.3723,
      "step": 565
    },
    {
      "epoch": 28.50632911392405,
      "grad_norm": 2.7422163486480713,
      "learning_rate": 4.3100000000000004e-05,
      "loss": 0.477,
      "step": 570
    },
    {
      "epoch": 28.759493670886076,
      "grad_norm": 4.201035499572754,
      "learning_rate": 4.26e-05,
      "loss": 0.4086,
      "step": 575
    },
    {
      "epoch": 29.0,
      "grad_norm": 3.573359727859497,
      "learning_rate": 4.21e-05,
      "loss": 0.3875,
      "step": 580
    },
    {
      "epoch": 29.253164556962027,
      "grad_norm": 2.56058669090271,
      "learning_rate": 4.16e-05,
      "loss": 0.3745,
      "step": 585
    },
    {
      "epoch": 29.50632911392405,
      "grad_norm": 2.967510938644409,
      "learning_rate": 4.11e-05,
      "loss": 0.384,
      "step": 590
    },
    {
      "epoch": 29.759493670886076,
      "grad_norm": 3.1659433841705322,
      "learning_rate": 4.0600000000000004e-05,
      "loss": 0.3545,
      "step": 595
    },
    {
      "epoch": 30.0,
      "grad_norm": 4.015511989593506,
      "learning_rate": 4.0100000000000006e-05,
      "loss": 0.5221,
      "step": 600
    },
    {
      "epoch": 30.253164556962027,
      "grad_norm": 4.676102638244629,
      "learning_rate": 3.960000000000001e-05,
      "loss": 0.459,
      "step": 605
    },
    {
      "epoch": 30.50632911392405,
      "grad_norm": 3.2369773387908936,
      "learning_rate": 3.91e-05,
      "loss": 0.4227,
      "step": 610
    },
    {
      "epoch": 30.759493670886076,
      "grad_norm": 2.8954575061798096,
      "learning_rate": 3.86e-05,
      "loss": 0.3356,
      "step": 615
    },
    {
      "epoch": 31.0,
      "grad_norm": 4.673282623291016,
      "learning_rate": 3.8100000000000005e-05,
      "loss": 0.3864,
      "step": 620
    },
    {
      "epoch": 31.253164556962027,
      "grad_norm": 3.029491424560547,
      "learning_rate": 3.76e-05,
      "loss": 0.325,
      "step": 625
    },
    {
      "epoch": 31.50632911392405,
      "grad_norm": 3.0295023918151855,
      "learning_rate": 3.71e-05,
      "loss": 0.4075,
      "step": 630
    },
    {
      "epoch": 31.759493670886076,
      "grad_norm": 5.839602947235107,
      "learning_rate": 3.66e-05,
      "loss": 0.5134,
      "step": 635
    },
    {
      "epoch": 32.0,
      "grad_norm": 3.3186967372894287,
      "learning_rate": 3.61e-05,
      "loss": 0.3084,
      "step": 640
    },
    {
      "epoch": 32.25316455696203,
      "grad_norm": 3.271657705307007,
      "learning_rate": 3.56e-05,
      "loss": 0.2953,
      "step": 645
    },
    {
      "epoch": 32.50632911392405,
      "grad_norm": 2.684757947921753,
      "learning_rate": 3.51e-05,
      "loss": 0.3094,
      "step": 650
    },
    {
      "epoch": 32.75949367088607,
      "grad_norm": 3.034318685531616,
      "learning_rate": 3.46e-05,
      "loss": 0.3805,
      "step": 655
    },
    {
      "epoch": 33.0,
      "grad_norm": 3.0959901809692383,
      "learning_rate": 3.41e-05,
      "loss": 0.5655,
      "step": 660
    },
    {
      "epoch": 33.25316455696203,
      "grad_norm": 3.5679028034210205,
      "learning_rate": 3.3600000000000004e-05,
      "loss": 0.3783,
      "step": 665
    },
    {
      "epoch": 33.50632911392405,
      "grad_norm": 2.4488513469696045,
      "learning_rate": 3.3100000000000005e-05,
      "loss": 0.3637,
      "step": 670
    },
    {
      "epoch": 33.75949367088607,
      "grad_norm": 4.91139554977417,
      "learning_rate": 3.26e-05,
      "loss": 0.4045,
      "step": 675
    },
    {
      "epoch": 34.0,
      "grad_norm": 3.8352322578430176,
      "learning_rate": 3.21e-05,
      "loss": 0.3609,
      "step": 680
    },
    {
      "epoch": 34.25316455696203,
      "grad_norm": 4.229318141937256,
      "learning_rate": 3.16e-05,
      "loss": 0.3759,
      "step": 685
    },
    {
      "epoch": 34.50632911392405,
      "grad_norm": 4.0295820236206055,
      "learning_rate": 3.1100000000000004e-05,
      "loss": 0.473,
      "step": 690
    },
    {
      "epoch": 34.75949367088607,
      "grad_norm": 3.744473934173584,
      "learning_rate": 3.06e-05,
      "loss": 0.4162,
      "step": 695
    },
    {
      "epoch": 35.0,
      "grad_norm": 2.654456377029419,
      "learning_rate": 3.01e-05,
      "loss": 0.2178,
      "step": 700
    },
    {
      "epoch": 35.25316455696203,
      "grad_norm": 2.8019862174987793,
      "learning_rate": 2.96e-05,
      "loss": 0.3198,
      "step": 705
    },
    {
      "epoch": 35.50632911392405,
      "grad_norm": 5.424245357513428,
      "learning_rate": 2.91e-05,
      "loss": 0.401,
      "step": 710
    },
    {
      "epoch": 35.75949367088607,
      "grad_norm": 4.607357501983643,
      "learning_rate": 2.86e-05,
      "loss": 0.2805,
      "step": 715
    },
    {
      "epoch": 36.0,
      "grad_norm": 4.743221282958984,
      "learning_rate": 2.8100000000000005e-05,
      "loss": 0.5167,
      "step": 720
    },
    {
      "epoch": 36.25316455696203,
      "grad_norm": 2.56506085395813,
      "learning_rate": 2.7600000000000003e-05,
      "loss": 0.3577,
      "step": 725
    },
    {
      "epoch": 36.50632911392405,
      "grad_norm": 3.956522226333618,
      "learning_rate": 2.7100000000000005e-05,
      "loss": 0.3516,
      "step": 730
    },
    {
      "epoch": 36.75949367088607,
      "grad_norm": 3.8929131031036377,
      "learning_rate": 2.6600000000000003e-05,
      "loss": 0.4673,
      "step": 735
    },
    {
      "epoch": 37.0,
      "grad_norm": 2.352356433868408,
      "learning_rate": 2.61e-05,
      "loss": 0.2631,
      "step": 740
    },
    {
      "epoch": 37.25316455696203,
      "grad_norm": 2.400522232055664,
      "learning_rate": 2.5600000000000002e-05,
      "loss": 0.2734,
      "step": 745
    },
    {
      "epoch": 37.50632911392405,
      "grad_norm": 4.303524971008301,
      "learning_rate": 2.51e-05,
      "loss": 0.3046,
      "step": 750
    },
    {
      "epoch": 37.75949367088607,
      "grad_norm": 3.5744128227233887,
      "learning_rate": 2.46e-05,
      "loss": 0.4465,
      "step": 755
    },
    {
      "epoch": 38.0,
      "grad_norm": 4.556077480316162,
      "learning_rate": 2.41e-05,
      "loss": 0.4053,
      "step": 760
    },
    {
      "epoch": 38.25316455696203,
      "grad_norm": 2.922311305999756,
      "learning_rate": 2.36e-05,
      "loss": 0.2811,
      "step": 765
    },
    {
      "epoch": 38.50632911392405,
      "grad_norm": 2.5623621940612793,
      "learning_rate": 2.3100000000000002e-05,
      "loss": 0.3226,
      "step": 770
    },
    {
      "epoch": 38.75949367088607,
      "grad_norm": 2.980527877807617,
      "learning_rate": 2.26e-05,
      "loss": 0.4104,
      "step": 775
    },
    {
      "epoch": 39.0,
      "grad_norm": 3.318556547164917,
      "learning_rate": 2.2100000000000002e-05,
      "loss": 0.3863,
      "step": 780
    },
    {
      "epoch": 39.25316455696203,
      "grad_norm": 4.528940200805664,
      "learning_rate": 2.16e-05,
      "loss": 0.2728,
      "step": 785
    },
    {
      "epoch": 39.50632911392405,
      "grad_norm": 4.347472190856934,
      "learning_rate": 2.11e-05,
      "loss": 0.4706,
      "step": 790
    },
    {
      "epoch": 39.75949367088607,
      "grad_norm": 3.460503339767456,
      "learning_rate": 2.06e-05,
      "loss": 0.3342,
      "step": 795
    },
    {
      "epoch": 40.0,
      "grad_norm": 3.9526116847991943,
      "learning_rate": 2.01e-05,
      "loss": 0.3163,
      "step": 800
    },
    {
      "epoch": 40.25316455696203,
      "grad_norm": 2.80462908744812,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 0.3921,
      "step": 805
    },
    {
      "epoch": 40.50632911392405,
      "grad_norm": 2.1175146102905273,
      "learning_rate": 1.91e-05,
      "loss": 0.4489,
      "step": 810
    },
    {
      "epoch": 40.75949367088607,
      "grad_norm": 3.4538187980651855,
      "learning_rate": 1.86e-05,
      "loss": 0.3014,
      "step": 815
    },
    {
      "epoch": 41.0,
      "grad_norm": 3.5645291805267334,
      "learning_rate": 1.81e-05,
      "loss": 0.2362,
      "step": 820
    }
  ],
  "logging_steps": 5,
  "max_steps": 1000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 50,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.1145704431616e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
